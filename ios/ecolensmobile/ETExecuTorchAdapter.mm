#import <Foundation/Foundation.h>
#import "ETExecuTorchAdapter.h"
#import <TargetConditionals.h>

#include <cmath>
#include <cctype>
#include <cstddef>
#include <cstdint>
#include <cstdlib>
#include <cstring>
#include <memory>
#include <string>
#include <vector>

#ifndef ET_ENABLE_EXECUTORCH_CPP
#define ET_ENABLE_EXECUTORCH_CPP 0
#endif

#if ET_ENABLE_EXECUTORCH_CPP && !TARGET_OS_SIMULATOR && __has_include(<executorch/extension/module/module.h>) && __has_include(<executorch/extension/tensor/tensor.h>)
#define ET_HAS_EXECUTORCH_CPP 1
#include <executorch/extension/module/module.h>
#include <executorch/extension/tensor/tensor.h>
#else
#define ET_HAS_EXECUTORCH_CPP 0
#endif

namespace {

struct AdapterModel {
  std::string modelPath;
  std::string tokenizerPath;
  std::string preset;

  bool initialized = false;

#if ET_HAS_EXECUTORCH_CPP
  std::unique_ptr<executorch::extension::Module> module;
#endif
};

static const char *dupCString(const std::string &value)
{
  char *copy = static_cast<char *>(std::malloc(value.size() + 1));
  if (!copy) {
    return nullptr;
  }
  std::memcpy(copy, value.c_str(), value.size() + 1);
  return copy;
}

static void setError(const char **errorMessage, const std::string &message)
{
  if (!errorMessage) {
    return;
  }
  *errorMessage = dupCString(message);
}

static std::string safeString(const char *value)
{
  return value ? std::string(value) : std::string();
}

static std::string runHeuristicInference(const float *input,
                                         int64_t inputSize,
                                         int32_t width,
                                         int32_t height,
                                         const char *labelHint,
                                         const std::string &preset)
{
  const std::string label = safeString(labelHint);

  double meanAbs = 0.0;
  if (input && inputSize > 0) {
    for (int64_t i = 0; i < inputSize; i++) {
      meanAbs += std::fabs(static_cast<double>(input[i]));
    }
    meanAbs /= static_cast<double>(inputSize);
  }

  int ecoScore = 56;
  if (!label.empty()) {
    std::string lowered = label;
    for (char &c : lowered) {
      c = static_cast<char>(std::tolower(static_cast<unsigned char>(c)));
    }
    if (lowered.find("reusable") != std::string::npos || lowered.find("steel") != std::string::npos ||
        lowered.find("glass") != std::string::npos) {
      ecoScore += 22;
    }
    if (lowered.find("single") != std::string::npos || lowered.find("plastic") != std::string::npos ||
        lowered.find("disposable") != std::string::npos) {
      ecoScore -= 16;
    }
  }
  if (preset == "strict") {
    ecoScore -= 6;
  } else if (preset == "optimistic") {
    ecoScore += 6;
  }

  const int tensorBonus = static_cast<int>(std::round((0.35 - std::min(meanAbs, 1.0)) * 12.0));
  ecoScore += tensorBonus;
  if (ecoScore < 1) {
    ecoScore = 1;
  }
  if (ecoScore > 99) {
    ecoScore = 99;
  }

  const int co2 = std::max(12, 160 - ecoScore);
  const double confidence = std::max(0.35, std::min(0.96, 0.52 + (static_cast<double>(ecoScore) / 200.0)));

  NSString *name = label.empty() ? @"Detected Item" : [NSString stringWithUTF8String:label.c_str()];
  NSString *category = ecoScore >= 70 ? @"reusable-item" : @"single-use-or-unknown";
  NSString *suggestion = ecoScore >= 70
    ? @"Keep reusing this item to reduce lifecycle impact."
    : @"Prefer reusable alternatives to lower footprint.";

  NSDictionary *jsonDict = @{
    @"title": @"On-device (adapter)",
    @"name": name,
    @"category": category,
    @"ecoScore": @(ecoScore),
    @"co2Gram": @(co2),
    @"confidence": @(confidence),
    @"summary": suggestion,
    @"suggestion": suggestion,
    @"explanation": @"Result generated by ETExecuTorchAdapter heuristic path. Replace with real ExecuTorch model outputs.",
    @"scoreFactors": @[
      @{ @"code": @"adapter_input", @"label": @"Adapter tensor signal", @"detail": @"Uses tensor statistics until real model decoding is wired.", @"delta": @0 }
    ],
    @"runtimeDebug": @{
      @"preset": [NSString stringWithUTF8String:preset.c_str()],
      @"inputSize": @(inputSize),
      @"width": @(width),
      @"height": @(height),
      @"meanAbs": @(meanAbs)
    }
  };

  NSError *serializationError = nil;
  NSData *data = [NSJSONSerialization dataWithJSONObject:jsonDict options:0 error:&serializationError];
  if (!data || serializationError) {
    return std::string("{\"name\":\"Detected Item\",\"category\":\"unknown\",\"ecoScore\":50,\"co2Gram\":100,\"confidence\":0.5,\"summary\":\"Fallback JSON serialization failed.\",\"explanation\":\"Adapter serialization fallback.\"}");
  }
  return std::string(reinterpret_cast<const char *>(data.bytes), data.length);
}

#if ET_HAS_EXECUTORCH_CPP
static std::string runExecuTorchInference(AdapterModel *model,
                                          const float *input,
                                          int64_t inputSize,
                                          int32_t width,
                                          int32_t height,
                                          const char *labelHint,
                                          const char **errorMessage)
{
  if (!model || !model->module) {
    setError(errorMessage, "ExecuTorch module is not initialized.");
    return std::string();
  }

  const std::vector<executorch::aten::SizesType> sizes = {
      static_cast<executorch::aten::SizesType>(1),
      static_cast<executorch::aten::SizesType>(3),
      static_cast<executorch::aten::SizesType>(height),
      static_cast<executorch::aten::SizesType>(width)};
  auto inputTensor = executorch::extension::from_blob(const_cast<float *>(input), sizes);
  if (!inputTensor) {
    setError(errorMessage, "Failed to create input tensor for ExecuTorch.");
    return std::string();
  }

  auto forwardResult = model->module->forward(inputTensor);
  if (!forwardResult.ok()) {
    setError(errorMessage, "ExecuTorch forward() returned an error.");
    return std::string();
  }

  const auto &outputs = forwardResult.get();
  if (outputs.empty() || !outputs[0].isTensor()) {
    setError(errorMessage, "ExecuTorch output is empty or not a tensor.");
    return std::string();
  }

  const auto outputTensor = outputs[0].toTensor();
  const float *scores = outputTensor.const_data_ptr<float>();
  const int64_t scoreCount = outputTensor.numel();
  if (!scores || scoreCount <= 0) {
    setError(errorMessage, "ExecuTorch output tensor is empty.");
    return std::string();
  }

  int64_t bestIndex = 0;
  float bestScore = scores[0];
  for (int64_t i = 1; i < scoreCount; i++) {
    if (scores[i] > bestScore) {
      bestScore = scores[i];
      bestIndex = i;
    }
  }

  const double confidence = std::max(0.05, std::min(0.99, 1.0 / (1.0 + std::exp(-static_cast<double>(bestScore)))));
  const int ecoScore = std::max(1, std::min(99, 50 + static_cast<int>(std::round(confidence * 40.0))));
  const int co2 = std::max(10, 165 - ecoScore);

  NSString *name = safeString(labelHint).empty() ? [NSString stringWithFormat:@"Class %lld", bestIndex]
                                                  : [NSString stringWithUTF8String:labelHint];
  NSDictionary *jsonDict = @{
    @"title": @"On-device (ExecuTorch)",
    @"name": name,
    @"category": @"executorch-output",
    @"ecoScore": @(ecoScore),
    @"co2Gram": @(co2),
    @"confidence": @(confidence),
    @"summary": @"Inference produced by ExecuTorch runtime output parser.",
    @"suggestion": @"Use model-specific class mapping for richer recommendations.",
    @"explanation": @"ExecuTorch runtime produced tensor outputs and adapter parsed top-1 logits.",
    @"scoreFactors": @[
      @{ @"code": @"executorch_top1", @"label": @"Top-1 Logit", @"detail": [NSString stringWithFormat:@"Top class index: %lld", bestIndex], @"delta": @0 }
    ],
    @"runtimeDebug": @{
      @"preset": [NSString stringWithUTF8String:model->preset.c_str()],
      @"inputSize": @(inputSize),
      @"width": @(width),
      @"height": @(height),
      @"outputSize": @(scoreCount),
      @"topIndex": @(bestIndex),
      @"topLogit": @(bestScore)
    }
  };

  NSError *serializationError = nil;
  NSData *data = [NSJSONSerialization dataWithJSONObject:jsonDict options:0 error:&serializationError];
  if (!data || serializationError) {
    setError(errorMessage, "Failed to serialize ExecuTorch output JSON.");
    return std::string();
  }
  return std::string(reinterpret_cast<const char *>(data.bytes), data.length);
}
#endif

} // namespace

extern "C" void *et_ecolens_create_model(const char *model_path,
                                          const char *tokenizer_path,
                                          const char *preset,
                                          const char **error_message) __attribute__((used, visibility("default")));

extern "C" void *et_ecolens_create_model(const char *model_path,
                                         const char *tokenizer_path,
                                         const char *preset,
                                         const char **error_message)
{
  if (error_message) {
    *error_message = nullptr;
  }

  if (!model_path || std::strlen(model_path) == 0) {
    setError(error_message, "model_path is required.");
    return nullptr;
  }

  @autoreleasepool {
    NSString *modelPath = [NSString stringWithUTF8String:model_path];
    BOOL exists = [[NSFileManager defaultManager] fileExistsAtPath:modelPath];
    if (!exists) {
      setError(error_message, "Model path does not exist on device.");
      return nullptr;
    }
  }

  std::unique_ptr<AdapterModel> model = std::make_unique<AdapterModel>();
  model->modelPath = safeString(model_path);
  model->tokenizerPath = safeString(tokenizer_path);
  model->preset = safeString(preset);
  if (model->preset.empty()) {
    model->preset = "balanced";
  }

#if ET_HAS_EXECUTORCH_CPP
  model->module = std::make_unique<executorch::extension::Module>(model->modelPath);
  auto loadStatus = model->module->load();
  if (loadStatus != executorch::runtime::Error::Ok) {
    setError(error_message, "ExecuTorch module load() failed. Check model compatibility and backend delegates.");
    return nullptr;
  }
#else
  // ExecuTorch C++ SDK is not linked in this build.
  // Fallback inference path remains available below.
#endif
  model->initialized = true;

  return model.release();
}

extern "C" const char *et_ecolens_run_inference(void *handle,
                                                  const float *input,
                                                  int64_t input_size,
                                                  int32_t width,
                                                  int32_t height,
                                                  const char *label_hint,
                                                  const char **error_message) __attribute__((used, visibility("default")));

extern "C" const char *et_ecolens_run_inference(void *handle,
                                                const float *input,
                                                int64_t input_size,
                                                int32_t width,
                                                int32_t height,
                                                const char *label_hint,
                                                const char **error_message)
{
  if (error_message) {
    *error_message = nullptr;
  }

  if (!handle) {
    setError(error_message, "Model handle is null.");
    return nullptr;
  }

  AdapterModel *model = static_cast<AdapterModel *>(handle);
  if (!model->initialized) {
    setError(error_message, "Model is not initialized.");
    return nullptr;
  }

  if (!input || input_size <= 0 || width <= 0 || height <= 0) {
    setError(error_message, "Invalid tensor input.");
    return nullptr;
  }

  std::string json;
#if ET_HAS_EXECUTORCH_CPP
  json = runExecuTorchInference(model, input, input_size, width, height, label_hint, error_message);
  if (json.empty()) {
    return nullptr;
  }
#else
  json = runHeuristicInference(input, input_size, width, height, label_hint, model->preset);
#endif
  return dupCString(json);
}

extern "C" void et_ecolens_destroy_model(void *handle) __attribute__((used, visibility("default")));

extern "C" void et_ecolens_destroy_model(void *handle)
{
  if (!handle) {
    return;
  }
  AdapterModel *model = static_cast<AdapterModel *>(handle);
  delete model;
}

extern "C" void et_ecolens_free_cstring(const char *ptr) __attribute__((used, visibility("default")));

extern "C" void et_ecolens_free_cstring(const char *ptr)
{
  if (!ptr) {
    return;
  }
  std::free(const_cast<char *>(ptr));
}
