#import <Foundation/Foundation.h>
#import <React/RCTBridgeModule.h>

@interface ExecuTorchRecognizer : NSObject <RCTBridgeModule>
@end

@implementation ExecuTorchRecognizer

RCT_EXPORT_MODULE(ExecuTorchRecognizer);

+ (BOOL)requiresMainQueueSetup
{
  return NO;
}

RCT_REMAP_METHOD(
  detectAndSummarize,
  detectAndSummarize:(NSDictionary *)payload
  resolver:(RCTPromiseResolveBlock)resolve
  rejecter:(RCTPromiseRejectBlock)reject
)
{
  if (![payload isKindOfClass:[NSDictionary class]]) {
    reject(@"E_BAD_PAYLOAD", @"Payload must be an object.", nil);
    return;
  }

  NSString *detectedLabel = payload[@"detectedLabel"];
  if (![detectedLabel isKindOfClass:[NSString class]] || detectedLabel.length == 0) {
    detectedLabel = @"Unlabeled item";
  }

  NSNumber *confidenceInput = payload[@"confidence"];
  NSNumber *confidence = @0.62;
  if ([confidenceInput isKindOfClass:[NSNumber class]]) {
    confidence = confidenceInput;
  }

  /*
   TODO(ExecuTorch):
   1) Decode imageBase64 into CVPixelBuffer/UIImage.
   2) Run vision model (YOLO or VLM encoder) via ExecuTorch runtime.
   3) Run LLM summarizer (or direct VLM decoding).
   4) Return structured output matching JS result schema.
  */

  resolve(@{
    @"title": @"On-device (skeleton)",
    @"name": detectedLabel,
    @"category": @"on-device-prototype",
    @"ecoScore": @55,
    @"co2Gram": @95,
    @"confidence": confidence,
    @"suggestion": @"Use a reusable alternative where possible.",
    @"altRecommendation": @"Switch to reusable options.",
    @"explanation": @"Native ExecuTorch module skeleton is wired. Replace stub with real inference output.",
    @"scoreFactors": @[
      @{
        @"code": @"skeleton_placeholder",
        @"label": @"Prototype inference",
        @"detail": @"Result is generated by iOS native skeleton module.",
        @"delta": @0
      }
    ],
    @"runtime": @{
      @"engine": @"on-device",
      @"source": @"ios-native-skeleton"
    }
  });
}

@end
